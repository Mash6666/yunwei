#!/usr/bin/env python3
"""
åŸºäºReactæœºåˆ¶çš„æ™ºèƒ½è¿ç»´åŠ©æ‰‹å·¥ä½œæµ
æ”¯æŒå¯¹è¯è·¯ç”±å’ŒæŒ‰éœ€æ‰§è¡Œç³»ç»Ÿæ£€æŸ¥
"""

import asyncio
import time
from typing import Dict, List, Any, Optional
from datetime import datetime
from enum import Enum

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

from states import OpsAssistantState, StateManager, SystemStatus
from monitoring import PrometheusClient
from remote_executor import RemoteExecutor
from analyzer import SystemAnalyzer
from conversation_router import conversation_router, IntentType
from logger_config import get_logger, error_logger, log_operation, log_performance
from langgraph_logger import langgraph_logger, log_langgraph_node, log_langgraph_transition

logger = get_logger(__name__)

class WorkflowType(Enum):
    """å·¥ä½œæµç±»å‹"""
    CHAT = "chat"
    SYSTEM_CHECK = "system_check"
    SYSTEM_INFO = "system_info"
    TROUBLESHOOT = "troubleshoot"

class ReactOpsAssistantGraph:
    """åŸºäºReactæœºåˆ¶çš„æ™ºèƒ½è¿ç»´åŠ©æ‰‹å·¥ä½œæµ"""

    def __init__(self):
        self.state_manager = StateManager()
        self.prometheus_client = PrometheusClient()
        self.system_analyzer = SystemAnalyzer()
        self.remote_executor = RemoteExecutor()
        self.logger = get_logger("react_ops_graph")

        # æ„å»ºå¤šä¸ªå·¥ä½œæµ
        self.graphs = {
            WorkflowType.CHAT: self._build_chat_graph(),
            WorkflowType.SYSTEM_CHECK: self._build_system_check_graph(),
            WorkflowType.SYSTEM_INFO: self._build_system_info_graph(),
            WorkflowType.TROUBLESHOOT: self._build_troubleshoot_graph()
        }

        # ç¼“å­˜çš„æŒ‡æ ‡æ•°æ®
        self._cached_metrics = None
        self._metrics_cache_time = 0

    def _build_chat_graph(self) -> StateGraph:
        """æ„å»ºå¯¹è¯å·¥ä½œæµ"""
        workflow = StateGraph(OpsAssistantState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("route_intent", self._route_intent)
        workflow.add_node("chat_response", self._chat_response)
        workflow.add_node("end_conversation", self._end_conversation)

        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("route_intent")

        # æ·»åŠ è¾¹
        workflow.add_edge("route_intent", "chat_response")
        workflow.add_edge("chat_response", "end_conversation")
        workflow.add_edge("end_conversation", END)

        # åˆ›å»ºcheckpointer
        checkpointer = MemorySaver()
        return workflow.compile(checkpointer=checkpointer)

    def _build_system_check_graph(self) -> StateGraph:
        """æ„å»ºç³»ç»Ÿæ£€æŸ¥å·¥ä½œæµï¼ˆå®Œæ•´çš„å·¡æ£€æµç¨‹ï¼‰"""
        workflow = StateGraph(OpsAssistantState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("collect_metrics", self._collect_metrics)
        workflow.add_node("analyze_system", self._analyze_system)
        workflow.add_node("generate_plan", self._generate_plan)
        workflow.add_node("execute_plan", self._execute_plan)
        workflow.add_node("report_results", self._report_results)
        workflow.add_node("handle_errors", self._handle_errors)

        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("collect_metrics")

        # æ·»åŠ æ¡ä»¶è¾¹
        workflow.add_conditional_edges(
            "collect_metrics",
            self._check_metrics_success,
            {
                "success": "analyze_system",
                "error": "handle_errors"
            }
        )

        workflow.add_conditional_edges(
            "analyze_system",
            self._check_analysis_result,
            {
                "needs_action": "generate_plan",
                "healthy": "report_results",
                "error": "handle_errors"
            }
        )

        workflow.add_conditional_edges(
            "generate_plan",
            self._check_plan_executable,
            {
                "execute": "execute_plan",
                "skip_execution": "report_results"
            }
        )

        workflow.add_edge("execute_plan", "report_results")
        workflow.add_edge("handle_errors", "report_results")
        workflow.add_edge("report_results", END)

        # åˆ›å»ºcheckpointer
        checkpointer = MemorySaver()
        return workflow.compile(checkpointer=checkpointer)

    def _build_system_info_graph(self) -> StateGraph:
        """æ„å»ºç³»ç»Ÿä¿¡æ¯æŸ¥è¯¢å·¥ä½œæµï¼ˆç®€åŒ–ç‰ˆæ£€æŸ¥ï¼‰"""
        workflow = StateGraph(OpsAssistantState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("collect_basic_metrics", self._collect_basic_metrics)
        workflow.add_node("provide_system_info", self._provide_system_info)

        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("collect_basic_metrics")

        # æ·»åŠ è¾¹
        workflow.add_edge("collect_basic_metrics", "provide_system_info")
        workflow.add_edge("provide_system_info", END)

        # åˆ›å»ºcheckpointer
        checkpointer = MemorySaver()
        return workflow.compile(checkpointer=checkpointer)

    def _build_troubleshoot_graph(self) -> StateGraph:
        """æ„å»ºæ•…éšœæ’æŸ¥å·¥ä½œæµ"""
        workflow = StateGraph(OpsAssistantState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("collect_relevant_metrics", self._collect_relevant_metrics)
        workflow.add_node("analyze_problem", self._analyze_problem)
        workflow.add_node("provide_solution", self._provide_solution)

        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("collect_relevant_metrics")

        # æ·»åŠ è¾¹
        workflow.add_edge("collect_relevant_metrics", "analyze_problem")
        workflow.add_edge("analyze_problem", "provide_solution")
        workflow.add_edge("provide_solution", END)

        # åˆ›å»ºcheckpointer
        checkpointer = MemorySaver()
        return workflow.compile(checkpointer=checkpointer)

    # ==================== èŠ‚ç‚¹å®ç° ====================

    @log_langgraph_node("route_intent")
    async def _route_intent(self, state: OpsAssistantState) -> OpsAssistantState:
        """è·¯ç”±ç”¨æˆ·æ„å›¾"""
        start_time = time.time()

        try:
            user_query = state.get("user_query", "")

            # åˆ†æç”¨æˆ·æ„å›¾
            intent_analysis = conversation_router.analyze_intent(user_query, state.get("context", {}))

            # å­˜å‚¨æ„å›¾åˆ†æç»“æœ
            state["intent_analysis"] = intent_analysis
            state["workflow_type"] = intent_analysis.intent_type.value

            # è®°å½•æ€§èƒ½
            end_time = time.time()
            log_performance("route_intent", start_time, end_time, {
                "intent_type": intent_analysis.intent_type.value,
                "confidence": intent_analysis.confidence
            })

            self.logger.info(f"æ„å›¾è·¯ç”±å®Œæˆ: {intent_analysis.intent_type.value}")

            return state

        except Exception as e:
            logger.error(f"æ„å›¾è·¯ç”±å¤±è´¥: {e}")
            state["error_message"] = f"æ„å›¾åˆ†æå¤±è´¥: {str(e)}"
            return state

    @log_langgraph_node("chat_response")
    async def _chat_response(self, state: OpsAssistantState) -> OpsAssistantState:
        """ç”Ÿæˆå¯¹è¯å“åº”ï¼ˆä¸æ‰§è¡Œç³»ç»Ÿæ£€æŸ¥ï¼‰"""
        start_time = time.time()

        try:
            user_query = state.get("user_query", "")
            intent_analysis = state.get("intent_analysis")

            # ç”ŸæˆèŠå¤©ä¸Šä¸‹æ–‡
            current_metrics = self._get_cached_metrics() if intent_analysis and intent_analysis.requires_metrics else None
            # ç¡®ä¿ä¼ é€’æœ‰æ•ˆçš„æ„å›¾åˆ†æå¯¹è±¡
            if not intent_analysis:
                # å¦‚æœæ„å›¾åˆ†æä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„èŠå¤©æ„å›¾
                from conversation_router import IntentAnalysis, IntentType
                intent_analysis = IntentAnalysis(
                    intent_type=IntentType.CHAT,
                    confidence=0.5,
                    requires_metrics=False,
                    requires_execution=False,
                    extracted_params={},
                    reasoning="é»˜è®¤èŠå¤©æ„å›¾"
                )
            chat_context = conversation_router.generate_chat_context(intent_analysis, current_metrics)

            # æ„å»ºå¯¹è¯æç¤º
            chat_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Linuxç³»ç»Ÿè¿ç»´åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

{chat_context}

ç”¨æˆ·é—®é¢˜ï¼š{user_query}

è¯·æä¾›ä¸“ä¸šã€å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚å¦‚æœæ˜¯æŠ€æœ¯é—®é¢˜ï¼Œè¯·æä¾›å…·ä½“çš„æ“ä½œå»ºè®®ã€‚
å¦‚æœç”¨æˆ·è¯¢é—®ç³»ç»ŸçŠ¶æ€ï¼Œè¯·åŸºäºå½“å‰æä¾›çš„æ•°æ®è¿›è¡Œåˆ†æã€‚
å¦‚æœéœ€è¦æ‰§è¡Œç³»ç»Ÿæ£€æŸ¥ï¼Œè¯·æŒ‡å¯¼ç”¨æˆ·ç‚¹å‡»"æ‰§è¡Œç³»ç»Ÿæ£€æŸ¥"æŒ‰é’®ã€‚
"""

            # è®°å½•LLMäº¤äº’å¼€å§‹
            langgraph_logger.log_llm_interaction(
                phase="chat_response",
                prompt=chat_prompt,
                response="",
                model_name="qwen-max"
            )

            # è°ƒç”¨LLMç”Ÿæˆå›å¤
            from langchain_core.messages import HumanMessage, SystemMessage

            messages = [
                SystemMessage(content=self.system_analyzer.system_prompt),
                HumanMessage(content=chat_prompt)
            ]

            llm_start_time = time.time()
            response = self.system_analyzer.llm.invoke(messages)
            llm_end_time = time.time()
            ai_response = response.content

            # è®°å½•LLMäº¤äº’å®Œæˆ
            langgraph_logger.log_llm_interaction(
                phase="chat_response",
                prompt=chat_prompt,
                response=ai_response,
                model_name="qwen-max",
                response_time=llm_end_time - llm_start_time
            )

            # æ›´æ–°çŠ¶æ€
            state["ai_response"] = ai_response
            state["response_type"] = "chat"

            # è®°å½•æ€§èƒ½
            end_time = time.time()
            log_performance("chat_response", start_time, end_time, {
                "response_length": len(ai_response),
                "llm_response_time": llm_end_time - llm_start_time
            })

            self.logger.info(f"å¯¹è¯å“åº”ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(ai_response)}")

            return state

        except Exception as e:
            logger.error(f"å¯¹è¯å“åº”ç”Ÿæˆå¤±è´¥: {e}")
            state["error_message"] = f"ç”Ÿæˆå›å¤å¤±è´¥: {str(e)}"
            return state

    @log_langgraph_node("end_conversation")
    async def _end_conversation(self, state: OpsAssistantState) -> OpsAssistantState:
        """ç»“æŸå¯¹è¯"""
        try:
            # è®°å½•å¯¹è¯å®Œæˆ
            user_query = state.get("user_query", "")
            ai_response = state.get("ai_response", "")

            langgraph_logger.log_conversation(
                user_query=user_query,
                ai_response=ai_response,
                node_sequence=["route_intent", "chat_response", "end_conversation"],
                success=not state.get("error_message"),
                error_message=state.get("error_message"),
                context_data={
                    "workflow_type": state.get("workflow_type"),
                    "response_type": state.get("response_type"),
                    "intent_analysis": state.get("intent_analysis").__dict__ if state.get("intent_analysis") else None
                }
            )

            return state

        except Exception as e:
            logger.error(f"ç»“æŸå¯¹è¯å¤±è´¥: {e}")
            return state

    # ==================== ç³»ç»Ÿæ£€æŸ¥å·¥ä½œæµèŠ‚ç‚¹====================

    @log_langgraph_node("collect_metrics")
    async def _collect_metrics(self, state: OpsAssistantState) -> OpsAssistantState:
        """æ”¶é›†ç›‘æ§æŒ‡æ ‡"""
        try:
            logger.info("å¼€å§‹æ”¶é›†ç›‘æ§æŒ‡æ ‡...")

            # è·å–PrometheusæŒ‡æ ‡
            metrics = self.prometheus_client.fetch_metrics()

            # æ£€æµ‹å‘Šè­¦
            alerts = self.prometheus_client.detect_alerts(metrics)

            # æ›´æ–°çŠ¶æ€
            self.state_manager.update_metrics(metrics)
            for alert in alerts:
                self.state_manager.add_alert(alert)

            # ç¼“å­˜æŒ‡æ ‡æ•°æ®
            self._cached_metrics = {
                "metrics": metrics,
                "alerts": alerts,
                "timestamp": time.time()
            }
            self._metrics_cache_time = time.time()

            # è®°å½•æ“ä½œ
            self.state_manager.add_action("collect_metrics", {
                "metrics_count": len(metrics),
                "alerts_count": len(alerts),
                "timestamp": datetime.now().isoformat()
            })

            logger.info(f"æ”¶é›†åˆ° {len(metrics)} ä¸ªæŒ‡æ ‡ï¼Œ{len(alerts)} ä¸ªå‘Šè­¦")
            return self.state_manager.get_state()

        except Exception as e:
            logger.error(f"æ”¶é›†ç›‘æ§æŒ‡æ ‡å¤±è´¥: {e}")
            state["error_message"] = f"ç›‘æ§æ•°æ®æ”¶é›†å¤±è´¥: {str(e)}"
            return state

    @log_langgraph_node("collect_basic_metrics")
    async def _collect_basic_metrics(self, state: OpsAssistantState) -> OpsAssistantState:
        """æ”¶é›†åŸºç¡€æŒ‡æ ‡ï¼ˆç”¨äºç³»ç»Ÿä¿¡æ¯æŸ¥è¯¢ï¼‰"""
        try:
            # å°è¯•ä»ç¼“å­˜è·å–æŒ‡æ ‡
            cached_data = self._get_cached_metrics()
            if cached_data:
                logger.info("ä½¿ç”¨ç¼“å­˜çš„æŒ‡æ ‡æ•°æ®")
                self.state_manager.update_metrics(cached_data["metrics"])
                for alert in cached_data["alerts"]:
                    self.state_manager.add_alert(alert)
                return self.state_manager.get_state()

            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œåˆ™æ”¶é›†æ–°æŒ‡æ ‡
            return await self._collect_metrics(state)

        except Exception as e:
            logger.error(f"æ”¶é›†åŸºç¡€æŒ‡æ ‡å¤±è´¥: {e}")
            state["error_message"] = f"åŸºç¡€æŒ‡æ ‡æ”¶é›†å¤±è´¥: {str(e)}"
            return state

    @log_langgraph_node("collect_relevant_metrics")
    async def _collect_relevant_metrics(self, state: OpsAssistantState) -> OpsAssistantState:
        """æ”¶é›†ç›¸å…³æŒ‡æ ‡ï¼ˆç”¨äºæ•…éšœæ’æŸ¥ï¼‰"""
        try:
            # æ•…éšœæ’æŸ¥é€šå¸¸éœ€è¦æœ€æ–°çš„æŒ‡æ ‡æ•°æ®
            return await self._collect_metrics(state)

        except Exception as e:
            logger.error(f"æ”¶é›†ç›¸å…³æŒ‡æ ‡å¤±è´¥: {e}")
            state["error_message"] = f"ç›¸å…³æŒ‡æ ‡æ”¶é›†å¤±è´¥: {str(e)}"
            return state

    @log_langgraph_node("provide_system_info")
    async def _provide_system_info(self, state: OpsAssistantState) -> OpsAssistantState:
        """æä¾›ç³»ç»Ÿä¿¡æ¯å“åº”"""
        try:
            user_query = state.get("user_query", "")
            intent_analysis = state.get("intent_analysis")
            metrics = state.get("metrics", [])
            alerts = state.get("alerts", [])

            # ç”Ÿæˆç³»ç»Ÿä¿¡æ¯æŠ¥å‘Š
            info_response = self._generate_system_info_report(user_query, intent_analysis, metrics, alerts)

            state["ai_response"] = info_response
            state["response_type"] = "system_info"

            return state

        except Exception as e:
            logger.error(f"ç”Ÿæˆç³»ç»Ÿä¿¡æ¯å¤±è´¥: {e}")
            state["error_message"] = f"ç³»ç»Ÿä¿¡æ¯ç”Ÿæˆå¤±è´¥: {str(e)}"
            return state

    @log_langgraph_node("analyze_problem")
    async def _analyze_problem(self, state: OpsAssistantState) -> OpsAssistantState:
        """åˆ†æé—®é¢˜"""
        try:
            user_query = state.get("user_query", "")
            intent_analysis = state.get("intent_analysis")
            metrics = state.get("metrics", [])
            alerts = state.get("alerts", [])

            # ä½¿ç”¨LLMåˆ†æé—®é¢˜
            problem_analysis = self._analyze_problem_with_llm(user_query, intent_analysis, metrics, alerts)

            state["problem_analysis"] = problem_analysis
            return state

        except Exception as e:
            logger.error(f"é—®é¢˜åˆ†æå¤±è´¥: {e}")
            state["error_message"] = f"é—®é¢˜åˆ†æå¤±è´¥: {str(e)}"
            return state

    @log_langgraph_node("provide_solution")
    async def _provide_solution(self, state: OpsAssistantState) -> OpsAssistantState:
        """æä¾›è§£å†³æ–¹æ¡ˆ"""
        try:
            user_query = state.get("user_query", "")
            problem_analysis = state.get("problem_analysis", "")

            # ç”Ÿæˆè§£å†³æ–¹æ¡ˆ
            solution_response = self._generate_solution_response(user_query, problem_analysis)

            state["ai_response"] = solution_response
            state["response_type"] = "solution"

            return state

        except Exception as e:
            logger.error(f"è§£å†³æ–¹æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            state["error_message"] = f"è§£å†³æ–¹æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}"
            return state

    # ==================== è¾…åŠ©æ–¹æ³• ====================

    def _get_cached_metrics(self) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜çš„æŒ‡æ ‡æ•°æ®"""
        if self._cached_metrics and self._metrics_cache_time:
            age = time.time() - self._metrics_cache_time
            if age < 300:  # 5åˆ†é’Ÿç¼“å­˜
                return self._cached_metrics
        return None

    def _generate_system_info_report(self, user_query: str, intent_analysis, metrics: List, alerts: List) -> str:
        """ç”Ÿæˆç³»ç»Ÿä¿¡æ¯æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        report = f"# ç³»ç»Ÿä¿¡æ¯æŠ¥å‘Š\n\n**ç”Ÿæˆæ—¶é—´**: {timestamp}\n\n"

        # æ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„å‚æ•°æä¾›ç‰¹å®šä¿¡æ¯
        if intent_analysis and intent_analysis.extracted_params:
            resource_type = intent_analysis.extracted_params.get("resource_type")
            if resource_type:
                report += f"## {intent_analysis.extracted_params.get('resource_name', resource_type.upper())} ä¿¡æ¯\n\n"
                # æ·»åŠ ç‰¹å®šèµ„æºçš„ä¿¡æ¯
                # ... è¿™é‡Œå¯ä»¥æ ¹æ®resource_typeæä¾›è¯¦ç»†ä¿¡æ¯

        # æ·»åŠ æ€»ä½“ç³»ç»ŸçŠ¶æ€
        report += "## ç³»ç»ŸçŠ¶æ€æ¦‚è§ˆ\n\n"
        report += f"- ç›‘æ§æŒ‡æ ‡æ•°é‡: {len(metrics)}\n"
        report += f"- æ´»è·ƒå‘Šè­¦æ•°é‡: {len(alerts)}\n"

        if metrics:
            report += "\n### å…³é”®æŒ‡æ ‡\n"
            # æ˜¾ç¤ºå‰å‡ ä¸ªå…³é”®æŒ‡æ ‡
            for metric in metrics[:5]:
                status_icon = "âœ…" if metric.status.value == 'normal' else "âš ï¸" if metric.status.value == 'warning' else "âŒ"
                report += f"- {status_icon} **{metric.name}**: {metric.value}{metric.unit}\n"

        if alerts:
            report += "\n### å½“å‰å‘Šè­¦\n"
            for alert in alerts[:3]:
                level_icon = "ğŸ”´" if alert.level.value == 'critical' else "ğŸŸ¡"
                report += f"- {level_icon} **{alert.metric_name}**: {alert.message}\n"

        report += f"\n---\n*æŠ¥å‘Šç”±æ™ºèƒ½è¿ç»´åŠ©æ‰‹è‡ªåŠ¨ç”Ÿæˆ*"

        return report

    def _analyze_problem_with_llm(self, user_query: str, intent_analysis, metrics: List, alerts: List) -> str:
        """ä½¿ç”¨LLMåˆ†æé—®é¢˜"""
        # æ„å»ºåˆ†ææç¤º
        analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç³»ç»Ÿé—®é¢˜ï¼š

ç”¨æˆ·æè¿°: {user_query}
æå–çš„å‚æ•°: {intent_analysis.extracted_params if intent_analysis else {}}

å½“å‰ç³»ç»ŸçŠ¶æ€:
- ç›‘æ§æŒ‡æ ‡æ•°é‡: {len(metrics)}
- æ´»è·ƒå‘Šè­¦æ•°é‡: {len(alerts)}

è¯·åˆ†æå¯èƒ½çš„é—®é¢˜åŸå› å¹¶æä¾›åˆæ­¥çš„è¯Šæ–­ç»“æœã€‚
"""

        # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
        from langchain_core.messages import HumanMessage, SystemMessage

        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç³»ç»Ÿæ•…éšœè¯Šæ–­ä¸“å®¶ã€‚"),
            HumanMessage(content=analysis_prompt)
        ]

        response = self.system_analyzer.llm.invoke(messages)
        return response.content

    def _generate_solution_response(self, user_query: str, problem_analysis: str) -> str:
        """ç”Ÿæˆè§£å†³æ–¹æ¡ˆå“åº”"""
        solution_prompt = f"""
åŸºäºä»¥ä¸‹é—®é¢˜åˆ†æï¼Œè¯·æä¾›è¯¦ç»†çš„è§£å†³æ–¹æ¡ˆï¼š

ç”¨æˆ·é—®é¢˜: {user_query}
é—®é¢˜åˆ†æ: {problem_analysis}

è¯·æä¾›ï¼š
1. é—®é¢˜çš„æ ¹æœ¬åŸå› 
2. å…·ä½“çš„è§£å†³æ­¥éª¤
3. é¢„é˜²æªæ–½
4. å¦‚æœéœ€è¦ï¼Œç›¸å…³çš„å‘½ä»¤ç¤ºä¾‹
"""

        from langchain_core.messages import HumanMessage, SystemMessage

        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç³»ç»Ÿé—®é¢˜è§£å†³ä¸“å®¶ã€‚"),
            HumanMessage(content=solution_prompt)
        ]

        response = self.system_analyzer.llm.invoke(messages)
        return response.content

    # ==================== çŠ¶æ€è½¬æ¢æ£€æŸ¥å‡½æ•° ====================

    @log_langgraph_transition("collect_metrics", "metrics_success_check")
    def _check_metrics_success(self, state: OpsAssistantState) -> str:
        """æ£€æŸ¥æŒ‡æ ‡æ”¶é›†æ˜¯å¦æˆåŠŸ"""
        if state.get("error_message"):
            return "error"
        if not state["metrics"]:
            return "error"
        return "success"

    @log_langgraph_transition("analyze_system", "analysis_result_check")
    def _check_analysis_result(self, state: OpsAssistantState) -> str:
        """æ£€æŸ¥åˆ†æç»“æœ"""
        if state.get("error_message"):
            return "error"

        analysis_result = state["context"].get("analysis_result", {})
        if not analysis_result:
            return "error"

        # å¦‚æœæœ‰æ£€æµ‹åˆ°é—®é¢˜ï¼Œéœ€è¦æ‰§è¡Œæ“ä½œ
        if analysis_result.get("detected_issues"):
            return "needs_action"

        # å¦‚æœç³»ç»Ÿå¥åº·ï¼Œç›´æ¥æŠ¥å‘Š
        return "healthy"

    @log_langgraph_transition("generate_plan", "plan_executable_check")
    def _check_plan_executable(self, state: OpsAssistantState) -> str:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œè®¡åˆ’"""
        if not state["execution_plan"]:
            return "skip_execution"

        analysis_result = state["context"].get("analysis_result", {})

        # å¦‚æœåˆ†æå»ºè®®è‡ªåŠ¨ä¿®å¤ï¼Œåˆ™æ‰§è¡Œ
        if analysis_result.get("auto_fixable", False):
            return "execute"

        # å¦‚æœæœ‰ä¸¥é‡é—®é¢˜ï¼Œä¹Ÿæ‰§è¡Œ
        urgency = analysis_result.get("urgency", "low")
        if urgency in ["high", "critical"]:
            return "execute"

        # å¦åˆ™è·³è¿‡æ‰§è¡Œï¼ŒåªæŠ¥å‘Š
        return "skip_execution"

    # ==================== å¤ç”¨åŸæœ‰èŠ‚ç‚¹ ====================

    @log_langgraph_node("analyze_system")
    async def _analyze_system(self, state: OpsAssistantState) -> OpsAssistantState:
        """åˆ†æç³»ç»ŸçŠ¶æ€"""
        try:
            logger.info("å¼€å§‹åˆ†æç³»ç»ŸçŠ¶æ€...")

            metrics = state["metrics"]
            alerts = state["alerts"]

            # ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½åˆ†æ
            analysis_result = self.system_analyzer.analyze_metrics(metrics, alerts)

            # è§£æJSONæ ¼å¼çš„åˆ†æç»“æœ
            parsed_result = self.system_analyzer._parse_analysis_result(analysis_result["raw_analysis"])

            # æ›´æ–°åˆ†æç»“æœ
            self.state_manager.update_analysis(
                analysis_result["raw_analysis"],
                parsed_result.get("issues", []),
                parsed_result
            )

            # å¦‚æœæœ‰ä¿®å¤è®¡åˆ’ï¼Œä¿å­˜åˆ°çŠ¶æ€ä¸­ï¼ˆä½†ä¿ç•™ç”¨æˆ·å·²ç»ç¼–è¾‘è¿‡çš„æ–¹æ¡ˆï¼‰
            if "fix_plans" in parsed_result:
                existing_plans = self.state_manager.state.get("fix_plans", [])

                # å¦‚æœç”¨æˆ·å·²ç»ç¼–è¾‘è¿‡æ–¹æ¡ˆï¼ˆæœ‰ä¿®æ”¹æ ‡è®°ï¼‰ï¼Œåˆ™ä¿ç•™ç”¨æˆ·ç¼–è¾‘çš„ç‰ˆæœ¬
                if existing_plans and any(plan.get("_user_edited", False) for plan in existing_plans):
                    logger.info("æ£€æµ‹åˆ°ç”¨æˆ·ç¼–è¾‘è¿‡çš„æ–¹æ¡ˆï¼Œä¿ç•™ç”¨æˆ·ç‰ˆæœ¬")
                    # ä¸è¦†ç›–ç”¨æˆ·ç¼–è¾‘è¿‡çš„æ–¹æ¡ˆ
                else:
                    # ä½¿ç”¨æ–°åˆ†æç”Ÿæˆçš„æ–¹æ¡ˆ
                    self.state_manager.set_fix_plans(parsed_result["fix_plans"])

            # è®°å½•æ“ä½œ
            self.state_manager.add_action("analyze_system", {
                "analysis_urgency": parsed_result.get("urgency", "medium"),
                "auto_fixable": parsed_result.get("auto_fixable", False),
                "issues_count": len(parsed_result.get("issues", [])),
                "fix_plans_count": len(parsed_result.get("fix_plans", [])),
                "timestamp": datetime.now().isoformat()
            })

            # ä¿å­˜åˆ†ææ•°æ®åˆ°ä¸Šä¸‹æ–‡
            state["context"]["analysis_result"] = parsed_result

            logger.info(f"ç³»ç»Ÿåˆ†æå®Œæˆï¼Œæ£€æµ‹åˆ° {len(parsed_result.get('issues', []))} ä¸ªé—®é¢˜ï¼Œ{len(parsed_result.get('fix_plans', []))} ä¸ªä¿®å¤è®¡åˆ’")
            return self.state_manager.get_state()

        except Exception as e:
            logger.error(f"ç³»ç»Ÿåˆ†æå¤±è´¥: {e}")
            state["error_message"] = f"ç³»ç»Ÿåˆ†æå¤±è´¥: {str(e)}"
            return state

    @log_langgraph_node("generate_plan")
    async def _generate_plan(self, state: OpsAssistantState) -> OpsAssistantState:
        """ç”Ÿæˆæ‰§è¡Œè®¡åˆ’"""
        try:
            logger.info("å¼€å§‹ç”Ÿæˆæ‰§è¡Œè®¡åˆ’...")

            analysis_result = state["context"].get("analysis_result", {})

            if not analysis_result:
                state["error_message"] = "ç¼ºå°‘åˆ†æç»“æœï¼Œæ— æ³•ç”Ÿæˆæ‰§è¡Œè®¡åˆ’"
                return state

            # ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
            execution_plan = self.system_analyzer.generate_execution_plan(analysis_result)

            # æ›´æ–°çŠ¶æ€
            self.state_manager.set_execution_plan(execution_plan)

            # è®°å½•æ“ä½œ
            self.state_manager.add_action("generate_plan", {
                "plan_commands": len(execution_plan),
                "auto_fix_recommended": analysis_result.get("auto_fixable", False),
                "urgency": analysis_result.get("urgency", "medium"),
                "timestamp": datetime.now().isoformat()
            })

            logger.info(f"ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼ŒåŒ…å« {len(execution_plan)} ä¸ªæ“ä½œ")
            return self.state_manager.get_state()

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ‰§è¡Œè®¡åˆ’å¤±è´¥: {e}")
            state["error_message"] = f"æ‰§è¡Œè®¡åˆ’ç”Ÿæˆå¤±è´¥: {str(e)}"
            return state

    @log_langgraph_node("execute_plan")
    async def _execute_plan(self, state: OpsAssistantState) -> OpsAssistantState:
        """æ‰§è¡Œè®¡åˆ’"""
        try:
            logger.info("å¼€å§‹æ‰§è¡Œè¿ç»´è®¡åˆ’...")

            execution_plan = state["execution_plan"]

            if not execution_plan:
                logger.info("æ²¡æœ‰éœ€è¦æ‰§è¡Œçš„æ“ä½œ")
                return state

            # è¿æ¥åˆ°è¿œç¨‹æœåŠ¡å™¨
            with self.remote_executor as executor:
                for i, command in enumerate(execution_plan):
                    logger.info(f"æ‰§è¡Œæ“ä½œ {i+1}/{len(execution_plan)}: {command}")

                    # æ‰§è¡Œå‘½ä»¤
                    result = executor.execute_command(command)

                    # è®°å½•ç»“æœ
                    self.state_manager.add_execution_result(result)

                    # å¦‚æœæ‰§è¡Œå¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ç»§ç»­æ‰§è¡Œå…¶ä»–å‘½ä»¤
                    if not result.success:
                        logger.warning(f"å‘½ä»¤æ‰§è¡Œå¤±è´¥: {command}, é”™è¯¯: {result.error}")

            # è®°å½•æ“ä½œ
            self.state_manager.add_action("execute_plan", {
                "commands_executed": len(execution_plan),
                "success_count": len([r for r in state["execution_results"] if r.success]),
                "timestamp": datetime.now().isoformat()
            })

            logger.info(f"æ‰§è¡Œè®¡åˆ’å®Œæˆï¼Œå…±æ‰§è¡Œ {len(execution_plan)} ä¸ªæ“ä½œ")
            return self.state_manager.get_state()

        except Exception as e:
            logger.error(f"æ‰§è¡Œè®¡åˆ’å¤±è´¥: {e}")
            state["error_message"] = f"è®¡åˆ’æ‰§è¡Œå¤±è´¥: {str(e)}"
            return state

    @log_langgraph_node("report_results")
    async def _report_results(self, state: OpsAssistantState) -> OpsAssistantState:
        """æŠ¥å‘Šç»“æœ"""
        try:
            logger.info("ç”Ÿæˆè¿ç»´æŠ¥å‘Š...")

            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            report = self._generate_report(state)

            # æ›´æ–°AIå“åº”
            state["ai_response"] = report
            state["response_type"] = "system_check"

            # è®°å½•æ“ä½œ
            self.state_manager.add_action("report_results", {
                "report_length": len(report),
                "system_status": state["system_status"].value,
                "timestamp": datetime.now().isoformat()
            })

            logger.info("è¿ç»´æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            return state

        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
            state["error_message"] = f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"
            return state

    @log_langgraph_node("handle_errors")
    async def _handle_errors(self, state: OpsAssistantState) -> OpsAssistantState:
        """å¤„ç†é”™è¯¯"""
        error_message = state.get("error_message", "æœªçŸ¥é”™è¯¯")
        logger.error(f"å¤„ç†é”™è¯¯: {error_message}")

        # æ›´æ–°ç³»ç»ŸçŠ¶æ€
        state["system_status"] = SystemStatus.CRITICAL

        # ç”Ÿæˆé”™è¯¯æŠ¥å‘Š
        error_report = f"""
## é”™è¯¯æŠ¥å‘Š

**é”™è¯¯ä¿¡æ¯**: {error_message}
**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

**å»ºè®®æ“ä½œ**:
1. æ£€æŸ¥ç³»ç»Ÿè¿æ¥çŠ¶æ€
2. éªŒè¯é…ç½®å‚æ•°
3. æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
4. æ‰‹åŠ¨æ£€æŸ¥ç³»ç»ŸçŠ¶æ€

**ç³»ç»ŸçŠ¶æ€**: ä¸¥é‡å¼‚å¸¸ï¼Œéœ€è¦äººå·¥å¹²é¢„
"""

        state["ai_response"] = error_report
        state["response_type"] = "error"

        # è®°å½•é”™è¯¯æ“ä½œ
        self.state_manager.add_action("handle_errors", {
            "error_message": error_message,
            "timestamp": datetime.now().isoformat()
        })

        return state

    def _generate_report(self, state: OpsAssistantState) -> str:
        """ç”Ÿæˆè¿ç»´æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        report = f"""
# æ™ºèƒ½è¿ç»´åŠ©æ‰‹æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {timestamp}
**ä¼šè¯ID**: {state['session_id']}

## ç³»ç»ŸçŠ¶æ€æ¦‚è§ˆ
**æ€»ä½“çŠ¶æ€**: {state['system_status'].value}
**ç›‘æ§æŒ‡æ ‡æ•°é‡**: {len(state['metrics'])}
**æ´»è·ƒå‘Šè­¦æ•°é‡**: {len(state['alerts'])}

## å…³é”®æŒ‡æ ‡
"""

        # æ·»åŠ å…³é”®æŒ‡æ ‡ä¿¡æ¯
        critical_metrics = [m for m in state['metrics'] if m.status.value in ['warning', 'critical']]
        if critical_metrics:
            report += "\n### å¼‚å¸¸æŒ‡æ ‡\n"
            for metric in critical_metrics[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå¼‚å¸¸æŒ‡æ ‡
                status_icon = "âŒ" if metric.status.value == 'critical' else "âš ï¸"
                report += f"- {status_icon} **{metric.name}**: {metric.value}{metric.unit}"
                if metric.threshold:
                    report += f" (é˜ˆå€¼: {metric.threshold})"
                report += "\n"

        # æ·»åŠ å‘Šè­¦ä¿¡æ¯
        if state['alerts']:
            report += "\n## æ´»è·ƒå‘Šè­¦\n"
            for alert in state['alerts'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªå‘Šè­¦
                level_icon = "ğŸ”´" if alert.level.value == 'critical' else "ğŸŸ¡"
                report += f"- {level_icon} **{alert.metric_name}**: {alert.message}\n"
                report += f"  - å½“å‰å€¼: {alert.value}, é˜ˆå€¼: {alert.threshold}\n"
                if alert.suggested_actions:
                    report += f"  - å»ºè®®æ“ä½œ: {', '.join(alert.suggested_actions[:2])}\n"

        # æ·»åŠ åˆ†æç»“æœ
        if state.get('analysis_result'):
            report += "\n## æ™ºèƒ½åˆ†æç»“æœ\n"
            report += state['analysis_result'] + "\n"

        # æ·»åŠ æ‰§è¡Œè®¡åˆ’
        if state['execution_plan']:
            report += "\n## è‡ªåŠ¨æ‰§è¡Œè®¡åˆ’\n"
            for i, command in enumerate(state['execution_plan'], 1):
                report += f"{i}. `{command}`\n"

        # æ·»åŠ æ‰§è¡Œç»“æœ
        if state['execution_results']:
            report += "\n## æ‰§è¡Œç»“æœ\n"
            success_count = len([r for r in state['execution_results'] if r.success])
            report += f"æˆåŠŸæ‰§è¡Œ: {success_count}/{len(state['execution_results'])} ä¸ªæ“ä½œ\n"

            # æ˜¾ç¤ºæœ€è¿‘çš„æˆåŠŸå’Œå¤±è´¥æ“ä½œ
            for result in state['execution_results'][-3:]:  # æ˜¾ç¤ºæœ€å3ä¸ªç»“æœ
                status_icon = "âœ…" if result.success else "âŒ"
                report += f"- {status_icon} `{result.command}`\n"
                if result.error:
                    report += f"  é”™è¯¯: {result.error}\n"

        # æ·»åŠ é”™è¯¯ä¿¡æ¯
        if state.get('error_message'):
            report += f"\n## âš ï¸ é”™è¯¯ä¿¡æ¯\n{state['error_message']}\n"

        report += f"\n---\n*æŠ¥å‘Šç”±æ™ºèƒ½è¿ç»´åŠ©æ‰‹è‡ªåŠ¨ç”Ÿæˆ*"

        return report

    # ==================== ä¸»è¦è¿è¡Œæ¥å£ ====================

    async def run(self, user_query: str = None) -> Dict[str, Any]:
        """è¿è¡ŒReactæ™ºèƒ½è¿ç»´åŠ©æ‰‹"""
        start_time = time.time()
        session_id = self.state_manager.state.get("session_id", f"react_session_{int(time.time())}")

        try:
            logger.info("å¯åŠ¨Reactæ™ºèƒ½è¿ç»´åŠ©æ‰‹...")

            # å¼€å§‹ä¼šè¯æ—¥å¿—
            langgraph_logger.start_session(session_id, user_query or "")

            # è®°å½•ç³»ç»Ÿæ“ä½œ
            langgraph_logger.log_system_action(
                "å¯åŠ¨Reactè¿ç»´åŠ©æ‰‹",
                {"user_query": user_query, "session_id": session_id}
            )

            # é‡ç½®çŠ¶æ€
            self.state_manager.reset_state()

            # è®¾ç½®ç”¨æˆ·æŸ¥è¯¢
            if user_query:
                self.state_manager.state["user_query"] = user_query

            # åˆ›å»ºåˆå§‹çŠ¶æ€
            initial_state = self.state_manager.get_state()

            # é¦–å…ˆåˆ†ææ„å›¾
            intent_analysis = conversation_router.analyze_intent(user_query or "")

            # æ ¹æ®æ„å›¾é€‰æ‹©å·¥ä½œæµ
            if intent_analysis.intent_type == IntentType.SYSTEM_CHECK:
                workflow = self.graphs[WorkflowType.SYSTEM_CHECK]
            elif intent_analysis.intent_type == IntentType.SYSTEM_INFO:
                workflow = self.graphs[WorkflowType.SYSTEM_INFO]
            elif intent_analysis.intent_type == IntentType.TROUBLESHOOT:
                workflow = self.graphs[WorkflowType.TROUBLESHOOT]
            else:
                # é»˜è®¤ä½¿ç”¨å¯¹è¯å·¥ä½œæµ
                workflow = self.graphs[WorkflowType.CHAT]

            self.logger.info(f"é€‰æ‹©å·¥ä½œæµ: {workflow}")

            # è¿è¡Œé€‰å®šçš„å·¥ä½œæµ
            config = {"configurable": {"thread_id": session_id}}
            final_state = await workflow.ainvoke(initial_state, config=config)

            # æ›´æ–°çŠ¶æ€ç®¡ç†å™¨
            self.state_manager.state.update(final_state)

            # è·å–èŠ‚ç‚¹æ‰§è¡Œåºåˆ—ï¼ˆä»æ—¥å¿—ä¸­æ”¶é›†ï¼‰
            node_sequence = langgraph_logger.node_stack.copy()

            # è®°å½•å¯¹è¯
            if user_query and final_state.get("ai_response"):
                self.state_manager.add_conversation(user_query, final_state["ai_response"])

                # è®°å½•åˆ°LangGraphå¯¹è¯æ—¥å¿—
                langgraph_logger.log_conversation(
                    user_query=user_query,
                    ai_response=final_state["ai_response"],
                    node_sequence=node_sequence,
                    success=not final_state.get("error_message"),
                    error_message=final_state.get("error_message"),
                    context_data={
                        "session_id": session_id,
                        "workflow_type": intent_analysis.intent_type.value,
                        "response_type": final_state.get("response_type"),
                        "system_status": final_state.get("system_status"),
                        "metrics_count": len(final_state.get("metrics", [])),
                        "alerts_count": len(final_state.get("alerts", []))
                    }
                )

            end_time = time.time()
            processing_time = end_time - start_time

            logger.info(f"Reactæ™ºèƒ½è¿ç»´åŠ©æ‰‹è¿è¡Œå®Œæˆ (è€—æ—¶: {processing_time:.2f}s)")

            # ç»“æŸä¼šè¯æ—¥å¿—
            langgraph_logger.end_session()

            return {
                "success": True,
                "state": final_state,
                "response": final_state.get("ai_response", ""),
                "summary": self.state_manager.get_summary(),
                "session_id": session_id,
                "processing_time": processing_time,
                "workflow_type": intent_analysis.intent_type.value,
                "response_type": final_state.get("response_type", "unknown")
            }

        except Exception as e:
            logger.error(f"Reactæ™ºèƒ½è¿ç»´åŠ©æ‰‹è¿è¡Œå¤±è´¥: {e}")

            # è®°å½•å¤±è´¥å¯¹è¯
            if user_query:
                langgraph_logger.log_conversation(
                    user_query=user_query,
                    ai_response=f"ç³»ç»Ÿé”™è¯¯: {str(e)}",
                    node_sequence=langgraph_logger.node_stack.copy(),
                    success=False,
                    error_message=str(e)
                )

            # ç»“æŸä¼šè¯æ—¥å¿—
            langgraph_logger.end_session()

            return {
                "success": False,
                "error": str(e),
                "state": self.state_manager.get_state(),
                "response": f"Reactæ™ºèƒ½è¿ç»´åŠ©æ‰‹è¿è¡Œå¤±è´¥: {str(e)}",
                "session_id": session_id
            }

    def get_current_state(self) -> OpsAssistantState:
        """è·å–å½“å‰çŠ¶æ€"""
        return self.state_manager.get_state()